{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12. Dimensionality reduction\n",
    "\n",
    "과제로 아래 cell을 작성해주세요\n",
    "\n",
    "주석의 경우 이미지, 테이블 등의 표현이 어려운 관계로 받지 않겠습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 실습과 같은 데이터를 사용합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 1. PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA를 사용하여 좌표계를 늘려가보세요\n",
    "\n",
    "그리고 좌표계 갯수의 변화에 따른 분류기의 성능이 어떻게 변하는지 보여주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "\n",
    "from plotly import express as px\n",
    "from plotly import graph_objs as go\n",
    "tf.config.get_visible_devices()\n",
    "\n",
    "train = pd.read_csv('12_train.csv')\n",
    "test = pd.read_csv('12_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "tr_x, tr_y = train.iloc[:,:-1], train.iloc[:,-1]\n",
    "ts_x, ts_y = test.iloc[:,:-1], test.iloc[:,-1]\n",
    "\n",
    "columns = tr_x.columns\n",
    "tr_x = standard_scaler.fit_transform(tr_x)\n",
    "ts_x = standard_scaler.transform(ts_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(tr_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66.2, 77.92, 79.787, 79.52, 79.84, 80.0, 80.147, 80.347, 79.987, 80.32]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "rf=RandomForestClassifier(n_jobs=-1)\n",
    "score =[]\n",
    "for i in range(1,11):\n",
    "    pca_tr_x=pca.transform(tr_x)[:,:i]\n",
    "    pca_ts_x=pca.transform(ts_x)[:,:i]\n",
    "    rf.fit(pca_tr_x,tr_y)\n",
    "    a = round(rf.score(pca_ts_x, ts_y)*100,2)\n",
    "    score.append(a)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array(score).reshape((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>66.2</td>\n",
       "      <td>77.92</td>\n",
       "      <td>79.787</td>\n",
       "      <td>79.52</td>\n",
       "      <td>79.84</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.147</td>\n",
       "      <td>80.347</td>\n",
       "      <td>79.987</td>\n",
       "      <td>80.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1      2       3      4      5     6       7       8       9     10\n",
       "score  66.2  77.92  79.787  79.52  79.84  80.0  80.147  80.347  79.987  80.32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(score, index = ['score'], columns=['1','2','3','4','5','6','7','8','9','10'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좌표계의 갯수가 늘어날수록 분류기의 성능이 좋아지는데 3개부터는 큰차이를 보이지 않고   \n",
    "9개에서는 성능이 떨어지는 결과를 얻었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 2. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영상보다 parameter가 많도록 autoencoder를 생성하고 학습시켜주세요\n",
    "\n",
    "encoder로 압축한 latent dimension에 대해 본인의 생각을 작성해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_shape,latent_dim):\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input((input_shape)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense((input_shape+latent_dim)//2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(tf.nn.leaky_relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense((input_shape+latent_dim)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(tf.nn.leaky_relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(latent_dim))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(input_shape, latent_dim):\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.Input((latent_dim)))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense((input_shape+latent_dim)//2))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(tf.nn.leaky_relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense((input_shape+latent_dim)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Activation(tf.nn.leaky_relu))\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "  \n",
    "    model.add(tf.keras.layers.Dense(input_shape))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_27 (InputLayer)        [(None, 23)]              0         \n",
      "_________________________________________________________________\n",
      "sequential_17 (Sequential)   (None, 3)                 913       \n",
      "_________________________________________________________________\n",
      "sequential_18 (Sequential)   (None, 23)                1193      \n",
      "=================================================================\n",
      "Total params: 2,106\n",
      "Trainable params: 1,950\n",
      "Non-trainable params: 156\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "enc=encoder(tr_x.shape[1],3)\n",
    "dec=decoder(tr_x.shape[1],3)\n",
    "\n",
    "input_layer=tf.keras.Input((tr_x.shape[1]))\n",
    "ae_output=dec(enc(input_layer))\n",
    "ae=tf.keras.models.Model(input_layer, ae_output)\n",
    "ae.compile('Adam','mse')\n",
    "\n",
    "ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 1.0243 - val_loss: 0.5898\n",
      "Epoch 2/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.7533 - val_loss: 0.5304\n",
      "Epoch 3/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6954 - val_loss: 0.5219\n",
      "Epoch 4/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6626 - val_loss: 0.5166\n",
      "Epoch 5/1000\n",
      "225/225 [==============================] - 0s 962us/step - loss: 0.6478 - val_loss: 0.5071\n",
      "Epoch 6/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6372 - val_loss: 0.5017\n",
      "Epoch 7/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6278 - val_loss: 0.4989\n",
      "Epoch 8/1000\n",
      "225/225 [==============================] - 0s 975us/step - loss: 0.6216 - val_loss: 0.4960\n",
      "Epoch 9/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6170 - val_loss: 0.4911\n",
      "Epoch 10/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.4853\n",
      "Epoch 11/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.4876\n",
      "Epoch 12/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6019 - val_loss: 0.4856\n",
      "Epoch 13/1000\n",
      "225/225 [==============================] - 0s 997us/step - loss: 0.6053 - val_loss: 0.4843\n",
      "Epoch 14/1000\n",
      "225/225 [==============================] - 0s 993us/step - loss: 0.6010 - val_loss: 0.4838\n",
      "Epoch 15/1000\n",
      "225/225 [==============================] - 0s 997us/step - loss: 0.6006 - val_loss: 0.4893\n",
      "Epoch 16/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.6014 - val_loss: 0.4835\n",
      "Epoch 17/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.4837\n",
      "Epoch 18/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.4799\n",
      "Epoch 19/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5964 - val_loss: 0.4838\n",
      "Epoch 20/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5945 - val_loss: 0.4822\n",
      "Epoch 21/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5940 - val_loss: 0.4808\n",
      "Epoch 22/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.4763\n",
      "Epoch 23/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5928 - val_loss: 0.4760\n",
      "Epoch 24/1000\n",
      "225/225 [==============================] - 0s 971us/step - loss: 0.5881 - val_loss: 0.4753\n",
      "Epoch 25/1000\n",
      "225/225 [==============================] - 0s 980us/step - loss: 0.5888 - val_loss: 0.4711\n",
      "Epoch 26/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5857 - val_loss: 0.4693\n",
      "Epoch 27/1000\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.590 - 0s 1ms/step - loss: 0.5841 - val_loss: 0.4674\n",
      "Epoch 28/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.4684\n",
      "Epoch 29/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5823 - val_loss: 0.4652\n",
      "Epoch 30/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 0.4632\n",
      "Epoch 31/1000\n",
      "225/225 [==============================] - 0s 984us/step - loss: 0.5816 - val_loss: 0.4550\n",
      "Epoch 32/1000\n",
      "225/225 [==============================] - 0s 997us/step - loss: 0.5789 - val_loss: 0.4588\n",
      "Epoch 33/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5800 - val_loss: 0.4578\n",
      "Epoch 34/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.4574\n",
      "Epoch 35/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5768 - val_loss: 0.4541\n",
      "Epoch 36/1000\n",
      "225/225 [==============================] - 0s 997us/step - loss: 0.5749 - val_loss: 0.4538\n",
      "Epoch 37/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5707 - val_loss: 0.4542\n",
      "Epoch 38/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5708 - val_loss: 0.4531\n",
      "Epoch 39/1000\n",
      "225/225 [==============================] - 0s 988us/step - loss: 0.5741 - val_loss: 0.4519\n",
      "Epoch 40/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5713 - val_loss: 0.4492\n",
      "Epoch 41/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.4512\n",
      "Epoch 42/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5733 - val_loss: 0.4504\n",
      "Epoch 43/1000\n",
      "225/225 [==============================] - 0s 971us/step - loss: 0.5664 - val_loss: 0.4522\n",
      "Epoch 44/1000\n",
      "225/225 [==============================] - 0s 971us/step - loss: 0.5695 - val_loss: 0.4482\n",
      "Epoch 45/1000\n",
      "225/225 [==============================] - 0s 966us/step - loss: 0.5687 - val_loss: 0.4492\n",
      "Epoch 46/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5677 - val_loss: 0.4496\n",
      "Epoch 47/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5704 - val_loss: 0.4529\n",
      "Epoch 48/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5702 - val_loss: 0.4479\n",
      "Epoch 49/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5725 - val_loss: 0.4556\n",
      "Epoch 50/1000\n",
      "225/225 [==============================] - 0s 988us/step - loss: 0.5721 - val_loss: 0.4506\n",
      "Epoch 51/1000\n",
      "225/225 [==============================] - 0s 980us/step - loss: 0.5682 - val_loss: 0.4514\n",
      "Epoch 52/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5650 - val_loss: 0.4480\n",
      "Epoch 53/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5677 - val_loss: 0.4514\n",
      "Epoch 54/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5694 - val_loss: 0.4509\n",
      "Epoch 55/1000\n",
      "225/225 [==============================] - 0s 980us/step - loss: 0.5678 - val_loss: 0.4478\n",
      "Epoch 56/1000\n",
      "225/225 [==============================] - 0s 988us/step - loss: 0.5642 - val_loss: 0.4488\n",
      "Epoch 57/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5734 - val_loss: 0.4470\n",
      "Epoch 58/1000\n",
      "225/225 [==============================] - 0s 975us/step - loss: 0.5686 - val_loss: 0.4455\n",
      "Epoch 59/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5668 - val_loss: 0.4469\n",
      "Epoch 60/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5674 - val_loss: 0.4555\n",
      "Epoch 61/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5689 - val_loss: 0.4471\n",
      "Epoch 62/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5708 - val_loss: 0.4491\n",
      "Epoch 63/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5695 - val_loss: 0.4475\n",
      "Epoch 64/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5654 - val_loss: 0.4516\n",
      "Epoch 65/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5707 - val_loss: 0.4522\n",
      "Epoch 66/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5664 - val_loss: 0.4537\n",
      "Epoch 67/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5623 - val_loss: 0.4472\n",
      "Epoch 68/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5649 - val_loss: 0.4481\n",
      "Epoch 69/1000\n",
      "225/225 [==============================] - 0s 971us/step - loss: 0.5673 - val_loss: 0.4486\n",
      "Epoch 70/1000\n",
      "225/225 [==============================] - 0s 962us/step - loss: 0.5697 - val_loss: 0.4484\n",
      "Epoch 71/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5664 - val_loss: 0.4508\n",
      "Epoch 72/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5651 - val_loss: 0.4511\n",
      "Epoch 73/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5688 - val_loss: 0.4472\n",
      "Epoch 74/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5626 - val_loss: 0.4512\n",
      "Epoch 75/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5680 - val_loss: 0.4562\n",
      "Epoch 76/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5633 - val_loss: 0.4457\n",
      "Epoch 77/1000\n",
      "225/225 [==============================] - ETA: 0s - loss: 0.577 - 0s 1ms/step - loss: 0.5684 - val_loss: 0.4502\n",
      "Epoch 78/1000\n",
      "225/225 [==============================] - 0s 1ms/step - loss: 0.5663 - val_loss: 0.4521\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b845e82d90>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae.fit(tr_x,tr_x,100,1000,\n",
    "      callbacks=[tf.keras.callbacks.EarlyStopping(patience=20)],\n",
    "      validation_data=(ts_x,ts_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder로 원래의 차원을 압축해서 새로운 latent dimension만드는 것은 데이터를 적게 쓴다는 점에서 좋은 기술이라고 생각이 된다.  \n",
    "그러나 latent dimension으로부터 다시 원래의 차원으로 만들때 학습에 사용한 weight들의 용량이 원래데이터를 압축하면서 절약한 용량보다  \n",
    "크다면 사용할 이유는 없다고 생각됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
